{
 "metadata": {
  "name": "",
  "signature": "sha256:ccbe2c10ebceba02cfc0afcec6ef80c4072196ff6df457acbce47c0b78f9f4e5"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Model Validation\n",
      "##Contents\n",
      "1. What is model validation?\n",
      "2. Accuracy of a model and limitations\n",
      "3. Validating Classifier Models\n",
      "4. Validating Regression Models"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##1. What is model validation?\n",
      "Suppose we have built a machine learning model, based on any of supervised, unsupervised or semi-supervised learning. After building a model based on some algorithms, the next natural thing to do would be to check how well it is doing. The answer to how well it is doing depends on what *metric* is being used to measure the performance on what *dataset*. Evaluating your model based on various relevant metrics and datasets is called model validation.\n",
      "\n",
      "###Evaluation Metrics\n",
      "Different machine learning tasks have different performance metrics. For instance, if you have a classifier model, then you would probably use the metrics *average accuracy*, *log-loss*, *precision-recall*, *sensitivity-specificity* etc."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##2. Accuracy of a model and limitations\n",
      "[Accuracy](https://en.wikipedia.org/wiki/Accuracy_and_precision) is often the starting point for analyzing the quality of a predictive model, as well as an obvious criterion for prediction. However, it is not the only metric or criterion to evaluate a model, as we'll see below.\n",
      "\n",
      "### Limitations\n",
      "There are several problems with using accuracy as a key metric. For one, it is totally possible to come up with a model that is completely useless and has zero predictive power, yet it has higher accuracy. This is known as the [Accuracy Paradox](https://en.wikipedia.org/wiki/Accuracy_paradox). Second would be the problem of [overfitting](https://en.wikipedia.org/wiki/Overfitting). While we are building our model, if we are blindly targeting accuracy, it would easily result in overfitting. Such a model would again be useless as it would not perform well on unseen or new data."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##3. Validating Classifier Models\n",
      "Classification is about predicting class labels given input data. In [*binary classification*](https://en.wikipedia.org/wiki/Binary_classification), there are two possible output classes. In [*multi-class classification*](https://en.wikipedia.org/wiki/Multiclass_classification), there are more than two possible classes. While post focuses on binary classification, all the metrics mentioned below can be extended to multi-class classification.\n",
      "\n",
      "###Classification Metrics\n",
      "We'll be using the following terminology when discussing the terms below:\n",
      "* True Positive (**TP**) - No. of observations correctly identified\n",
      "* True Negative (**TN**) - No. of observations correctly rejected\n",
      "* False Positive (**FP**) - No. of observations incorrectly identified\n",
      "* False Negative (**FN**) - No. of observations incorrectly rejected\n",
      "\n",
      "If it was a binary classification problem where the classes were either *0* or *1*, then identified could be thought of as *1* and rejected could be thought of as *0*."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": []
    }
   ],
   "metadata": {}
  }
 ]
}