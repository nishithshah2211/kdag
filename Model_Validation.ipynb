{
 "metadata": {
  "name": "",
  "signature": "sha256:0b4415df9bdfb34f74639072919025ced9afaa2966093465cf1762a6ececd416"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Model Validation, KS Test and Lorenz Curve\n",
      "##Contents\n",
      "1. What is model validation?\n",
      "2. Accuracy of a model and limitations\n",
      "3. Validating Classifier Models\n",
      "4. Validating Regression Models\n",
      "5. Kolmogorov Smirnov Test\n",
      "6. Lorenz Curve"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##1. What is model validation?\n",
      "Suppose we have built a machine learning model, based on any of supervised, unsupervised or semi-supervised learning. After building a model based on some algorithms, the next natural thing to do would be to check how well it is doing. The answer to how well it is doing depends on what *metric* is being used to measure the performance on what *dataset*. Evaluating your model based on various relevant metrics and datasets is called model validation.\n",
      "\n",
      "###Evaluation Metrics\n",
      "Different machine learning tasks have different performance metrics. For instance, if you have a classifier model, then you would probably use the metrics *average accuracy*, *log-loss*, *precision-recall*, *sensitivity-specificity* etc."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##2. Accuracy of a model and limitations\n",
      "[Accuracy](https://en.wikipedia.org/wiki/Accuracy_and_precision) is often the starting point for analyzing the quality of a predictive model, as well as an obvious criterion for prediction. However, it is not the only metric or criterion to evaluate a model, as we'll see below.\n",
      "\n",
      "### Limitations\n",
      "There are several problems with using accuracy as a key metric. For one, it is totally possible to come up with a model that is completely useless and has zero predictive power, yet it has higher accuracy. This is known as the [Accuracy Paradox](https://en.wikipedia.org/wiki/Accuracy_paradox). Second would be the problem of [overfitting](https://en.wikipedia.org/wiki/Overfitting). While we are building our model, if we are blindly targeting accuracy, it would easily result in overfitting. Such a model would again be useless as it would not perform well on unseen or new data. A third scenario comes up when the data you is highly skewed. In such cases, a high accuracy is bound to be achieved and it does not reflect anything on the quality of the model."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##3. Validating Classifier Models\n",
      "Classification is about predicting class labels given input data. In [*binary classification*](https://en.wikipedia.org/wiki/Binary_classification), there are two possible output classes. In [*multi-class classification*](https://en.wikipedia.org/wiki/Multiclass_classification), there are more than two possible classes. While post focuses on binary classification, all the metrics mentioned below can be extended to multi-class classification.\n",
      "\n",
      "###Classification Metrics\n",
      "\n",
      "We'll be using the following terminology when discussing the terms below:\n",
      "\n",
      "* True Positive (**TP**) - No. of observations correctly identified\n",
      "* True Negative (**TN**) - No. of observations correctly rejected\n",
      "* False Positive (**FP**) - No. of observations incorrectly identified\n",
      "* False Negative (**FN**) - No. of observations incorrectly rejected\n",
      "\n",
      "If it was a binary classification problem where the classes were either *0* or *1*, then identified could be thought of as *1* and rejected could be thought of as *0*."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "####Accuracy\n",
      "Accuracy is simply the ratio of the correct predictions to the total number of predictions.\n",
      "\n",
      "$$acc = \\frac {TP + TN}{TP + TN + FP + FN}$$\n",
      "\n",
      "Accuracy does not make any distinction between correct predictions for class *0* and class *1*. Most of the times, this is not enough or satisfactory."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "####[Confusion Matrix](https://en.wikipedia.org/wiki/Confusion_matrix)\n",
      "Contrary to its name, a confusion matrix shows a more detailed breakdown of the correct and incorrect classifications for each class. It provides us with some more details than a single accuracy figure, like how a model is performing w.r.t to False Positives(**FP**) and False Negatives(**FN**).\n",
      "\n",
      "![Confusion Matrix](cm.png \"Confusion Matrix\")\n",
      "\n",
      "From the above figure, we can define the following terms:\n",
      "\n",
      "* $$acc = \\frac {TP + TN}{TP + TN + FP + FN} \\ (as \\ defined \\ above)$$\n",
      "* $$Misclassification \\ Rate = \\frac {FP + FN}{TP + TN + FP + FN}$$\n",
      "* $$Sensitivity \\ (True \\ Positive \\ Rate) = \\frac {TP}{TP + FN}$$\n",
      "* $$Specificity \\ (True \\ Negative \\ Rate) = \\frac {TN}{TN + FP}$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### [Sensitivity and Specificity](https://en.wikipedia.org/wiki/Sensitivity_and_specificity)\n",
      "These are finer statistical measures of performance of a model of a binary classification test, defined as follows:\n",
      "\n",
      "**Sensitivity**(or **true positive rate**) - It is the proportion of the positives that are correctly identified, i.e., probability that an observation will be classified as *positive* by the model, given that it is *positive*.\n",
      "\n",
      "**Specificity**(or **true negative rate**) - It is the proportion of the negatives that are correctly identified, i.e., probability that an observation will be classified as *negative* by the model, given that it is *negative*.\n",
      "\n",
      "Sensitivity is a measure of how well *false negatives* are being avoided and similarly, specificity is a measure of how well *false positives* are avoided. An ideal model would be one that has $100\\%$ *sensitivity* as well as $100\\%$ *specificity*. However, every predictor model will possess a minimum error rate, known as the [Bayes Error Rate](https://en.wikipedia.org/wiki/Bayes_error_rate). So, usually its a trade-off between these two measures, depending on the domain of the problem. For instance, if we are building a model that predicts if a person is going to test positive for a virus or not, we would prefer if the model had high sensitivity as compared to high specificity because identifying the people that are positive for a virus would be a far more important task. In this case, we would not mind if our model predicts positive for a person who is actually negative for that virus with a reasonable error in exchange for some gain in sensitivity!\n",
      "\n",
      "Clearly, these measures provide more insight into the classifier model than just looking naively at accuracy."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The above metrics are some of the many metrics used to measure a classifier's performance.\n",
      "\n",
      "Further reading: [Receiver Operating Characteristic (ROC) Curve](https://en.wikipedia.org/wiki/Receiver_operating_characteristic)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##4. Validating Regression Models\n",
      "Broadly speaking, [regression](https://en.wikipedia.org/wiki/Regression_analysis) is the process of estimating relationship among different variables. Regression validation is the process of checking or deciding if the relationships among different variables obtained from regression analysis is acceptable or not, if it makes sense or not.\n",
      "\n",
      "###Regression Metrics\n",
      "\n",
      "####[Coefficient of Determination or $R^2$](https://en.wikipedia.org/wiki/Coefficient_of_determination)\n",
      "$R^2$ is a statistic that will give some information regarding the goodness fit of a regression model. It is an quantitative measure of how close the regression line is to the actual line. An $R^2$ value of 1 indicates that the regression model perfectly fits the data.\n",
      "\n",
      "Mathematically, we have the following:\n",
      "$$\\bar{y} = \\frac {1}{n}\\sum_{i=1}^{n}y_i$$\n",
      "\n",
      "We define the following terms, using which we will define $R^2$:\n",
      "\n",
      "$SS_{tot}$ is the *total sum of squares*.\n",
      "\n",
      "$SS_{tot} = \\sum_i(y_i - \\bar{y})^2$\n",
      "\n",
      "$SS_{res}$ is the *residual sum of squares*.\n",
      "\n",
      "$SS_{res} = \\sum_i(y_i - \\hat{y_i})^2$\n",
      "\n",
      "Now we define $R^2$ as:\n",
      "\n",
      "$R^2 = 1 - \\frac{SS_{res}}{SS_{tot}}$\n",
      "\n",
      "There are some things we should be aware of, regarding $R^2$. This metric is used when the model is built using [ordinary least squares](https://en.wikipedia.org/wiki/Least-squares) regression, i.e., by minimizing $SS_{res}$. In this case, as we increase the number of variables in the model, $R^2$ increases. So, a higher value of $R^2$ does not necessarily mean that the model fits the data well. Apart from that, $R^2$ does not indicate the following:\n",
      "\n",
      "* the correct *regression* relation was used;\n",
      "* the most appropriate set of *independent* variables was used;\n",
      "* there is collinearity between the data and the explanatory variables used;\n",
      "* the number of observations are sufficient to establish a relationship among the variables.\n",
      "\n",
      "Further reading: [Analysis of residuals](https://en.wikipedia.org/wiki/Regression_validation#Analysis_of_residuals)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "####Root Mean Squared Error(RMSE)\n",
      "RMSE is the most commonly used metric for regression tasks. It is a distance measure between the predicted numeric target and the actual numeric answer (ground truth). The smaller the value of the RMSE, the better is the predictive accuracy of the model. A model with perfectly correct predictions would have an RMSE of 0. Mathematically, it is defined as:\n",
      "$$RMSE = \\sqrt{\\frac {\\sum_i(y_i - \\hat{y_i})^2}{n}}$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###[Cross Validation](https://en.wikipedia.org/wiki/Cross-validation_%28statistics%29)\n",
      "Cross validation is a model validation technique for assessing the fit of a model on a new unseen data set. The purpose of cross validation is to hold out a part of the training data set and use it to test the model in training phase, so as to limit overfitting.\n",
      "\n",
      "There are several variants of cross validation. The most commonly used variant is [*k-fold cross validation*](https://en.wikipedia.org/wiki/Cross-validation_%28statistics%29#k-fold_cross-validation). In this variation, the training set is randomly partitioned into *k* equal sized subsets. Of the *k* subsets, a single subset is retained or held out as the testing or validation subset and the remaining *k-1* subsets are used to train the model. This process is repeated *k* times, with each subset being used as a hold out set exactly once.\n",
      "\n",
      "A small value of *k* would mean higher selection bias but lower variance in the performance and a higher value of *k* would lead to a lower selection bias but high variance in the performance. Wikipedia mentions that $k=10$ is commonly used."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##5. [Kolmogorov Smirnov test(KS test)](https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test)\n",
      "The **KS test** is used to compare the equality of a single sample against a reference distribution(one-sample KS test) or to compare the equality of two samples against one another(two-sample KS test). The *Kolmogorow Smirnov statistic* quantifies a distance between the [empirical distribution function](https://en.wikipedia.org/wiki/Empirical_distribution_function) of the sample and the cumulative distribution function of the reference distribution in case of one-sample KS test, or between the empirical distribution functions of two samples in case of two-sample KS test.\n",
      "\n",
      "###Empirical Distribution Function and KS Statistic\n",
      "The empirical distribution function $F_n$ for $n$ observations $X_i$ is defined as:\n",
      "$$F_n(x) = \\frac {1}{n} \\sum_{i=1}^{n}I_x(X_i)$$\n",
      "where $I_x$ is an indicator function which is 1 if $X_i \\leq x$ and 0 otherwise.\n",
      "\n",
      "The KS Statistic for one-sample KS test is defined as:\n",
      "$$D_n = sup|{F_n(x) - F(x)}|$$\n",
      "where $F(x)$ is the cumulative distribution function of the reference distribution and *sup* is the supremum function. \n",
      "\n",
      "We can conclude the null hypothesis that the sample comes from distribution $F(x)$ if $D_n$ follows the *Kolmogorov Distribtution* as described [here](https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test#Kolmogorov_distribution).\n",
      "\n",
      "The KS Statistic for two-sample KS test is defined as:\n",
      "$$D_{n, n'} = sup|F_{1, n}(x) - F_{2, n'}(x)|$$\n",
      "where $F_{1, n}$ and $F_{2, n'}$ are the empirical distribution functions of the first and second sample respectively and *sup* is the supremum function.\n",
      "\n",
      "We can conclude the null hypothesis that the first and second sample are drawn from the same distribution as long as:\n",
      "$$D_{n, n'} \\leq c(\\alpha)\\sqrt{\\frac {n + n'}{nn'}}$$\n",
      "The values of $\\alpha$ and $c(\\alpha)$ are given in the table below:\n",
      "\n",
      "|$\\alpha$      |0.10 | 0.05 | 0.025 | 0.01 | 0.005 | 0.001 |\n",
      "|--------------|-----|------|-------|------|-------|-------|\n",
      "|$c(\\alpha)$   |1.22 | 1.36 | 1.48  | 1.63 | 1.73  | 1.95  |\n",
      "\n",
      "Note that this does not specify the common distribution or give any information about the common distribution that $F_{1, n}$ and $F_{2, n'}$ might share.\n",
      "\n",
      "One limitation of KS test is that it applies only to continuous distributions."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}