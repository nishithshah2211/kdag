{
 "metadata": {
  "name": "",
  "signature": "sha256:bc55537a5491ae24ef143026df94e1331f71628181d50e61b515bfcf837e685d"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Model Validation\n",
      "##Contents\n",
      "1. What is model validation?\n",
      "2. Accuracy of a model and limitations\n",
      "3. Validating Classifier Models\n",
      "4. Validating Regression Models"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##1. What is model validation?\n",
      "Suppose we have built a machine learning model, based on any of supervised, unsupervised or semi-supervised learning. After building a model based on some algorithms, the next natural thing to do would be to check how well it is doing. The answer to how well it is doing depends on what *metric* is being used to measure the performance on what *dataset*. Evaluating your model based on various relevant metrics and datasets is called model validation.\n",
      "\n",
      "###Evaluation Metrics\n",
      "Different machine learning tasks have different performance metrics. For instance, if you have a classifier model, then you would probably use the metrics *average accuracy*, *log-loss*, *precision-recall*, *sensitivity-specificity* etc."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##2. Accuracy of a model and limitations\n",
      "[Accuracy](https://en.wikipedia.org/wiki/Accuracy_and_precision) is often the starting point for analyzing the quality of a predictive model, as well as an obvious criterion for prediction. However, it is not the only metric or criterion to evaluate a model, as we'll see below.\n",
      "\n",
      "### Limitations\n",
      "There are several problems with using accuracy as a key metric. For one, it is totally possible to come up with a model that is completely useless and has zero predictive power, yet it has higher accuracy. This is known as the [Accuracy Paradox](https://en.wikipedia.org/wiki/Accuracy_paradox). Second would be the problem of [overfitting](https://en.wikipedia.org/wiki/Overfitting). While we are building our model, if we are blindly targeting accuracy, it would easily result in overfitting. Such a model would again be useless as it would not perform well on unseen or new data. A third scenario comes up when the data you is highly skewed. In such cases, a high accuracy is bound to be achieved and it does not reflect anything on the quality of the model."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##3. Validating Classifier Models\n",
      "Classification is about predicting class labels given input data. In [*binary classification*](https://en.wikipedia.org/wiki/Binary_classification), there are two possible output classes. In [*multi-class classification*](https://en.wikipedia.org/wiki/Multiclass_classification), there are more than two possible classes. While post focuses on binary classification, all the metrics mentioned below can be extended to multi-class classification.\n",
      "\n",
      "###Classification Metrics\n",
      "We'll be using the following terminology when discussing the terms below:\n",
      "* True Positive (**TP**) - No. of observations correctly identified\n",
      "* True Negative (**TN**) - No. of observations correctly rejected\n",
      "* False Positive (**FP**) - No. of observations incorrectly identified\n",
      "* False Negative (**FN**) - No. of observations incorrectly rejected\n",
      "\n",
      "If it was a binary classification problem where the classes were either *0* or *1*, then identified could be thought of as *1* and rejected could be thought of as *0*."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "####Accuracy\n",
      "Accuracy is simply the ratio of the correct predictions to the total number of predictions.\n",
      "\n",
      "$$acc = \\frac {TP + TN}{TP + TN + FP + FN}$$\n",
      "\n",
      "Accuracy does not make any distinction between correct predictions for class *0* and class *1*. Most of the times, this is not enough or satisfactory."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "####[Confusion Matrix](https://en.wikipedia.org/wiki/Confusion_matrix)\n",
      "Contrary to its name, a confusion matrix shows a more detailed breakdown of the correct and incorrect classifications for each class. It provides us with some more details than a single accuracy figure, like how a model is performing w.r.t to False Positives(**FP**) and False Negatives(**FN**).\n",
      "\n",
      "![Confusion Matrix](cm.png \"Confusion Matrix\")\n",
      "\n",
      "From the above figure, we can define the following terms:\n",
      "\n",
      "* $$acc = \\frac {TP + TN}{TP + TN + FP + FN} \\ (as \\ defined \\ above)$$\n",
      "* $$Misclassification \\ Rate = \\frac {FP + FN}{TP + TN + FP + FN}$$\n",
      "* $$Sensitivity \\ (True \\ Positive \\ Rate) = \\frac {TP}{TP + FN}$$\n",
      "* $$Specificity \\ (True \\ Negative \\ Rate) = \\frac {TN}{TN + FP}$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### [Sensitivity and Specificity](https://en.wikipedia.org/wiki/Sensitivity_and_specificity)\n",
      "These are finer statistical measures of performance of a model of a binary classification test, defined as follows:\n",
      "\n",
      "**Sensitivity**(or **true positive rate**) - It is the proportion of the positives that are correctly identified, i.e., probability that an observation will be classified as *positive* by the model, given that it is *positive*.\n",
      "\n",
      "**Specificity**(or **true negative rate**) - It is the proportion of the negatives that are correctly identified, i.e., probability that an observation will be classified as *negative* by the model, given that it is *negative*.\n",
      "\n",
      "Sensitivity is a measure of how well *false negatives* are being avoided and similarly, specificity is a measure of how well *false positives* are avoided. An ideal model would be one that has $100\\%$ *sensitivity* as well as $100\\%$ *specificity*. However, every predictor model will possess a minimum error rate, known as the [Bayes Error Rate](https://en.wikipedia.org/wiki/Bayes_error_rate). So, usually its a trade-off between these two measures, depending on the domain of the problem. For instance, if we are building a model that predicts if a person is going to test positive for a virus or not, we would prefer if the model had high sensitivity as compared to high specificity because identifying the people that are positive for a virus would be a far more important task. In this case, we would not mind if our model predicts positive for a person who is actually negative for that virus with a reasonable error in exchange for some gain in sensitivity!\n",
      "\n",
      "Clearly, these measures provide more insight into the classifier model than just looking naively at accuracy."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}